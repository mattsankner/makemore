{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba134d4-70a4-4f3f-8b14-ccca44e52d27",
   "metadata": {},
   "source": [
    "# Makemore: Bigram Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad1c5b-400b-4cef-aece-1c9632b7e23a",
   "metadata": {},
   "source": [
    "### In this notebook, we will develop a bigram character language model using a neural network approach. The neural network will take a single character as input, process it through the network with weights/parameters ```w```, and output the character most likely to follow the input character in the sequence.\n",
    "\n",
    "- Additionally, we will evaluate different parameter settings of the neural network using the loss function, specifically the ```negative log likelihood.``` By examining the ```probability distributions``` and using labels (which represent the next character in the bigram), we can determine how accurately the model predicts the following character. Our goal is to achieve a high probability for the correct next character, resulting in a low loss.\n",
    "\n",
    "- We will employ ```gradient-based optimization``` to fine-tune the network. By minimizing the loss function, we will adjust the network's weights/parameters to accurately predict the probabilities of the subsequent character in the sequence.\n",
    "\n",
    "### Start by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da207c4-3a20-4b50-b163-f955bff85211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7f416b-813f-4d62-95ab-fba34372bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load names.text for reading into a massive string. Then move it into a python list of strings\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a764f02-ad2e-4c98-9164-c49cd43f3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words)))) #make words one big string of chars, and then into a list of sorted, unique chars\n",
    "stoi = {s : i+1 for i,s in enumerate(chars)} #string to integer map, shift values for '.'\n",
    "stoi['.'] = 0\n",
    "itos = {i : s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d2d61-3aed-4c46-a09f-7f86c157c05d",
   "metadata": {},
   "source": [
    "### We're going to start by compiling a training set of the bigram for the neural network.\n",
    "\n",
    "The training set will be made up of two lists: the inputs, and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0039c152-0e95-4a28-86a4-b37c6560b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], [] #inputs and targets\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs x= ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "#create tensors out of this\n",
    "xs = torch.tensor((xs)) #not torch.Tensor(which would cast as float) \n",
    "ys = torch.tensor((ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445aac92-8a38-4df4-9c96-f6a27f7fd24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b8d8eff-5e3c-4e19-9fb6-11854726bea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0ee37-7e9b-4221-b830-8c04c6d442f1",
   "metadata": {},
   "source": [
    "### How do we read this?\n",
    "\n",
    "- There are 5 bigrams in the input word ```emma```.\n",
    "- When the input integer is $0$ in ```xs```, which maps to input ```'.'``` in ```stoi```, the desired ```label``` is integer $5$ in ```ys```, which corresponds to ```e```.\n",
    "- Thus, when the input to the neural network is integer $5$, we want to adjust the parameters so integer $13$ gets a very high probability, and so on.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876df369-a741-48b4-81f8-271aa0009087",
   "metadata": {},
   "source": [
    "### Feeding Integers into Neural Networks: One-Hot Encoding\n",
    "\n",
    "- Right now, we're plugging in an integer input that represents the index of the character, which you can't really input to a neural network. It doesn't really make sense to feed in an input of integers to the neuron to be multiplied by the weights. So we should use a different way of encoding the integers.\n",
    "- In ```one-hot encoding```, instead of passing the integer $13$, we would pass in a vector with the $13th$ index filled with a $1$, and the rest of the indexes filled with $0's$.\n",
    "- ```PyTorch``` has a ```one-hot encoding``` function that takes a tensor made of integers, and ```num_classes```, which is how long you want your tensor to be. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e78172c0-d082-4d46-8ffa-ea208d6d7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "383137e0-3977-4be3-ad9b-0e853a9cfdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xencoded = F.one_hot(xs, num_classes=27).float() #ca sted to float because doesn't take dtype input\n",
    "xencoded #xs has 5 examples, set to length 27 each. 5 examples coded into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6d8273d-ed46-41af-9916-bbd6caaec124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xencoded.dtype) #automatically an integer becasue we passed xs which are 64 bit integers, but now a float\n",
    "xencoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6b69057-040d-45d0-9af6-5ac78fc5b66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11fbb8390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xencoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2dc3c1-3f06-4205-94b6-5bb45fcadd21",
   "metadata": {},
   "source": [
    "### Let's make our first neuron. The neurons will perform a simple function ```w(x) + b```, where ```w(x)``` is a dot product.\n",
    "\n",
    "Let's initialize the weights for the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d36099-640c-4ac8-9fef-23af4d835017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1636],\n",
       "        [0.6641],\n",
       "        [1.5792],\n",
       "        [1.5792],\n",
       "        [1.0138]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27,1)) #column vector, where 1 indicates the presence of 1 single neuron\n",
    "\n",
    "#Now, weights are multiplied by the inputs\n",
    "#take the one-hot encoding and multiply it by w, with matrix mult. or greater in pytorch\n",
    "\n",
    "xencoded @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411797fc-a0b9-4c11-80a0-4165fd2a8479",
   "metadata": {},
   "source": [
    "The operation ```xencoded @ W ```performs matrix multiplication to evaluate the dot product for each input example with all neurons. This is done in parallel, allowing efficient computation of probabilities for the next character in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d82c14-d1e7-46ae-91dc-77559aadfbf0",
   "metadata": {},
   "source": [
    "### Basically, we took ```x-encoding```, which is $5x27$, and we multiplied it by $27x1$. The output will become $5x1$, because the $27$ will multiply and add.\n",
    "\n",
    "- We're seeing the $5$ activations of the ```W``` nueron on the $5$ inputs from ```xencoded```. We inputed all $5$ inputs to the same neuron and evaluated them (using ```w(x) + b``` in parallel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983a170-a71c-4694-8160-9d48f44c3529",
   "metadata": {},
   "source": [
    "### Now, we want to create $27$ neurons, so we can evaluate all 27 neurons on all 5 inputs in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c7dcc80-2e39-4d2e-b314-70367fd6bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4820, -1.1094, -0.8787,  0.5043,  0.3617,  1.2164,  0.5823, -1.7402,\n",
      "         -1.4040, -1.5237,  0.9622,  1.0588, -0.6319, -1.4658, -1.2474,  0.5097,\n",
      "          1.1323,  0.9731, -0.3893, -0.2970, -0.6915, -0.5404,  1.3155,  1.6806,\n",
      "         -1.8419, -0.4272, -0.1364],\n",
      "        [-1.4081,  0.9108, -0.9205, -0.3495,  0.3606,  0.2921, -0.8392, -0.9323,\n",
      "         -1.6665,  1.1338,  0.3525,  0.0339,  0.2599, -0.5840, -0.5575, -0.1619,\n",
      "         -0.8952,  1.8736, -1.6432, -1.2224, -0.5003, -0.9029,  1.4006,  2.0920,\n",
      "          1.9899,  0.8264, -0.7327],\n",
      "        [-0.8857, -0.5182, -0.8138, -0.8223,  0.2427, -0.6459, -1.4622,  2.3162,\n",
      "         -0.5067,  0.1217,  1.6059,  0.1532,  1.1719,  0.1739,  0.8752, -0.1940,\n",
      "         -0.4199,  0.3347,  1.1409,  0.4607,  0.5146, -0.8147,  0.0952,  0.0900,\n",
      "          1.1077, -1.0880, -0.1864],\n",
      "        [-0.8857, -0.5182, -0.8138, -0.8223,  0.2427, -0.6459, -1.4622,  2.3162,\n",
      "         -0.5067,  0.1217,  1.6059,  0.1532,  1.1719,  0.1739,  0.8752, -0.1940,\n",
      "         -0.4199,  0.3347,  1.1409,  0.4607,  0.5146, -0.8147,  0.0952,  0.0900,\n",
      "          1.1077, -1.0880, -0.1864],\n",
      "        [-0.9189, -0.4932,  0.9569, -0.5478, -0.8038, -0.1184,  1.4706,  0.5676,\n",
      "         -0.8019, -0.4076, -0.0670, -0.3302, -0.0844, -2.0683,  1.5012,  0.2870,\n",
      "         -0.6828,  1.1905, -1.5036, -1.9685,  0.9151,  0.4580, -0.1033, -0.3040,\n",
      "          1.8657, -1.8660, -1.3535]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27,27)) #27 neurons\n",
    "print(xencoded @ W) #now multiplying 5x27 * 27*27 neuron, which output 5x27\n",
    "(xencoded @ W).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1e6e4-6c86-4191-b5a3-fdb55b4da4b5",
   "metadata": {},
   "source": [
    "### This operation is essentially telling us, \"for the 27 neurons we made, what is the firing rate of those neurons on every one of the $5$ of those examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b47108bb-9574-4788-b187-be2acef7fc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1739)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xencoded @ W)[3,13] #retrieves value from matrix product; the firing rate of the 13th neuron, looking at the 3rd input\n",
    "                    #achieved using the product of the 3rd input and the 13th column of the W matrix (activation of 13th neuron for the 3rd input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff0f9d-b9f8-4cf1-9b92-18d5e74ffacb",
   "metadata": {},
   "source": [
    "### Using matrix multiplication, we can very efficiently evaluate the dot product using lots of input examples in the batch, and lots of neurons, where all of those neurons have weights in the columns of those W's. We're doing those dot products in parallel.\n",
    "\n",
    "To explore this, we can take the $3rd$ row of ```xencoded``` and the $13th$ column of W, and can element-wise multiply them, and then sum them up. Finally, we check this against the original result of:\n",
    "```python\n",
    "(xencoded @ W)[3,13]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afc1e732-c26d-41a8-beb9-1e0bca1cbac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xencoded[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ca55ab0-6d6a-4651-bd4f-c87e51cc89d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4658, -2.0683, -1.4208,  1.7738, -0.2589, -0.5840, -0.3703, -0.8808,\n",
       "         1.0854,  0.3849,  1.1077, -0.8994, -0.5985,  0.1739, -0.7326, -1.5873,\n",
       "         2.1938, -0.8535, -0.4942, -0.8401, -1.1109, -0.7709, -0.5695,  0.6647,\n",
       "         0.3222,  0.0155, -0.2095])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06d0be66-8f3f-4ca7-8d90-e7a2b965e93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1739)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xencoded[3] * W[:,13]).sum() #w(x) + b without the b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9d0fa-43f9-4df4-a5d9-a504f7017da8",
   "metadata": {},
   "source": [
    "### Our answer is the same as ```(xencoded @ W)[3,13]``` because each element at the resulting matrix at position ```[i,j]``` is the ```dot-product``` of the ```ith``` row of matrix ```xencoded``` and the ```jth``` column of matrix ```W```.\n",
    "\n",
    "Now, we have a $27$ dimensional input and a first-layer (a linear) of $27$ neurons. These neurons don't have a ```bias```, and don't have a ```non-linearity``` like ```tanh()```. It's just a linear layer.\n",
    "\n",
    "### For every single input example, we're trying to produce a probability distribution for the next character in a sequence ($27$ of them). But how are we going to interpret the $27$ numbers these neurons take on?\n",
    "\n",
    "In our previous bigram model, we had a matrix of normalized probabilites derived from the counts of the character sequences. Right now, all we have are some negative and positive numbers, which we want to represent the probabilities for the next character.\n",
    "\n",
    "Since it's difficult to output counts (integers), and probabilities (positive numbers that add up to 1) out of a neural network, we can interpet the $27$ numbers as ```log(counts)```, where we take the counts and exponentiate them. \n",
    "\n",
    "As a result, the numbers in the output tensor that were below $0$ will be below $1$, but still positive, and those above $0$ will turn into even more positive numebrs, i/e higher than $1$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33269e59-75fa-4831-b4e6-1e22d9cedda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.4019,  0.3298,  0.4153,  1.6558,  1.4358,  3.3752,  1.7902,  0.1755,\n",
       "          0.2456,  0.2179,  2.6175,  2.8828,  0.5316,  0.2309,  0.2873,  1.6649,\n",
       "          3.1029,  2.6462,  0.6776,  0.7430,  0.5008,  0.5825,  3.7265,  5.3687,\n",
       "          0.1585,  0.6523,  0.8725],\n",
       "        [ 0.2446,  2.4863,  0.3983,  0.7050,  1.4342,  1.3393,  0.4320,  0.3937,\n",
       "          0.1889,  3.1074,  1.4226,  1.0345,  1.2968,  0.5577,  0.5726,  0.8506,\n",
       "          0.4085,  6.5117,  0.1934,  0.2945,  0.6064,  0.4054,  4.0578,  8.1012,\n",
       "          7.3149,  2.2851,  0.4806],\n",
       "        [ 0.4124,  0.5956,  0.4432,  0.4394,  1.2747,  0.5242,  0.2317, 10.1374,\n",
       "          0.6025,  1.1294,  4.9822,  1.1656,  3.2282,  1.1899,  2.3995,  0.8237,\n",
       "          0.6571,  1.3975,  3.1296,  1.5852,  1.6729,  0.4428,  1.0999,  1.0942,\n",
       "          3.0273,  0.3369,  0.8299],\n",
       "        [ 0.4124,  0.5956,  0.4432,  0.4394,  1.2747,  0.5242,  0.2317, 10.1374,\n",
       "          0.6025,  1.1294,  4.9822,  1.1656,  3.2282,  1.1899,  2.3995,  0.8237,\n",
       "          0.6571,  1.3975,  3.1296,  1.5852,  1.6729,  0.4428,  1.0999,  1.0942,\n",
       "          3.0273,  0.3369,  0.8299],\n",
       "        [ 0.3990,  0.6107,  2.6036,  0.5782,  0.4476,  0.8883,  4.3516,  1.7640,\n",
       "          0.4485,  0.6652,  0.9352,  0.7188,  0.9191,  0.1264,  4.4871,  1.3324,\n",
       "          0.5052,  3.2886,  0.2223,  0.1397,  2.4970,  1.5810,  0.9018,  0.7379,\n",
       "          6.4604,  0.1547,  0.2583]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xencoded @ W).exp() # e^x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085cbf9b-ebd3-45b1-be09-679aa1ea0235",
   "metadata": {},
   "source": [
    "### Exp() outputs give us something we can interpret as the equivalent of counts originally. ```xencoded``` values can take on various values depending on the settings of ```W```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2019be5-2d29-4f02-a532-983dd6a6ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1066, 0.0080, 0.0101, 0.0401, 0.0348, 0.0817, 0.0434, 0.0043, 0.0059,\n",
       "         0.0053, 0.0634, 0.0698, 0.0129, 0.0056, 0.0070, 0.0403, 0.0751, 0.0641,\n",
       "         0.0164, 0.0180, 0.0121, 0.0141, 0.0903, 0.1300, 0.0038, 0.0158, 0.0211],\n",
       "        [0.0052, 0.0528, 0.0085, 0.0150, 0.0304, 0.0284, 0.0092, 0.0084, 0.0040,\n",
       "         0.0659, 0.0302, 0.0220, 0.0275, 0.0118, 0.0122, 0.0180, 0.0087, 0.1382,\n",
       "         0.0041, 0.0062, 0.0129, 0.0086, 0.0861, 0.1719, 0.1552, 0.0485, 0.0102],\n",
       "        [0.0092, 0.0133, 0.0099, 0.0098, 0.0284, 0.0117, 0.0052, 0.2260, 0.0134,\n",
       "         0.0252, 0.1111, 0.0260, 0.0720, 0.0265, 0.0535, 0.0184, 0.0147, 0.0312,\n",
       "         0.0698, 0.0353, 0.0373, 0.0099, 0.0245, 0.0244, 0.0675, 0.0075, 0.0185],\n",
       "        [0.0092, 0.0133, 0.0099, 0.0098, 0.0284, 0.0117, 0.0052, 0.2260, 0.0134,\n",
       "         0.0252, 0.1111, 0.0260, 0.0720, 0.0265, 0.0535, 0.0184, 0.0147, 0.0312,\n",
       "         0.0698, 0.0353, 0.0373, 0.0099, 0.0245, 0.0244, 0.0675, 0.0075, 0.0185],\n",
       "        [0.0105, 0.0161, 0.0685, 0.0152, 0.0118, 0.0234, 0.1144, 0.0464, 0.0118,\n",
       "         0.0175, 0.0246, 0.0189, 0.0242, 0.0033, 0.1180, 0.0350, 0.0133, 0.0865,\n",
       "         0.0058, 0.0037, 0.0657, 0.0416, 0.0237, 0.0194, 0.1699, 0.0041, 0.0068]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the following are all differentiable operations\n",
    "logits = xencoded @ W # w(x) interpreted as log-counts \n",
    "counts = logits.exp() #Equivalent to the N array of counts in previous model; we exp() to get something that looks like counts\n",
    "probs = counts / counts.sum(1, keepdim=True) #counts normalized; sum counts along first dimension with keepdims = True\n",
    "                                            #normalize rows of counts matrix to get probabilties in a probability distribution\n",
    "print(probs[4].sum())\n",
    "probs #every row ( all 5 of them) is normalized/sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6700196-ae39-4222-ac55-4103ab8f8faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc835d72-b4aa-4738-b1a8-e247bcf22ecb",
   "metadata": {},
   "source": [
    "### For every one of our five examples, we now have a row that came out of a neural network.\n",
    "\n",
    "- Because of the transformations here, we made sure that the output of the neural network now are probabilities. Our ```w(x)``` gave us ```logits```, and we interpret those to become ```log(counts)```. We exponentiate to get something that looks like ```counts,``` and then we normalize those counts to get a probability distribution. And all of those are differentiable operations.\n",
    "\n",
    "### In summary, we are taking inputs, we have differentiable operations that we can backpropogate through, and we're getting out porbability distributions. Take, for example, the $0th$ example that was fed in, which was a ```one-hot vector``` of $0$ (the first yellow  block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c146e8d-bad2-42a9-a831-2f6de8f32e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x121c70350>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xencoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdc1aa-8dbe-4b17-8d14-e4101166da49",
   "metadata": {},
   "source": [
    "### The $0th$ example corresponded to feeding in a ```'.'``` to the neural network. We did this by:\n",
    "- first getting its index:\n",
    "  ```python\n",
    "  xs.append(ix1)\n",
    "  ```\n",
    "- one-hot encoding it:\n",
    "  ```python\n",
    "  xencoded = F.one_hot(xs, num_classes=27).float()\n",
    "  ```\n",
    "\n",
    "- put it through the neural net (The one-hot encoded input vector for the 0th example is multiplied with the weight matrix W):\n",
    "  ```python\n",
    "  xencoded @ W\n",
    "  ```\n",
    "- and outputted ```probs[0]``` distribution of probabilities, with a shape of $27$ numbers. We interpret these as the nn's assignment for how likely every one of the $27$ characters are likely to come next.\n",
    "\n",
    "### As we tune the weights W, we will get different probabilities out, for every character that we input. Now, the question is if we can optimize and finally get W such that the probabilities are pretty good that are outputted. We judge this by the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58d5d5-d178-4b6f-ab1c-78aae77bafe9",
   "metadata": {},
   "source": [
    "### Nice Job! If you don't understand it all right away, that's okay. A good and concise summary with code will be provided at the beginning of the next lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
