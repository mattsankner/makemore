{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7ae0d3-d709-469a-a0c2-bb0ab658d786",
   "metadata": {},
   "source": [
    "# Bigram Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44830e8c-9dd3-4bae-8326-dd3c22b204b0",
   "metadata": {},
   "source": [
    "### Below is a summary of what we've done with neural network implementation for the bigram model so far with code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e11117-455b-4368-a407-93a0dd2f5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c555a8e-4618-442c-8f2f-0b3d1ef069a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95628ad-d9ba-43f5-96d3-a346809b4ba3",
   "metadata": {},
   "source": [
    "### We have an input dataset and map indices to unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7729c0c-7779-4aa3-b9e1-718bd08f3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load names.text for reading into a massive string\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8653c3-5237-400b-a2c3-678e4730099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words)))) #make words one big string of chars, and then into a list of sorted, unique chars\n",
    "stoi = {s : i+1 for i,s in enumerate(chars)} #string to integer map, shift values for '.'\n",
    "stoi['.'] = 0\n",
    "itos = {i : s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da543a-1268-4d88-85b6-8161cc072234",
   "metadata": {},
   "source": [
    "### We take an example input to the neural net, the first word in the dataset, ```emma```, and add each character's integer mapping to a list of inputs to the neural net and a list of labels for the correct next character in the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c81b80-2a45-4d96-a37f-95e8afe28200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n"
     ]
    }
   ],
   "source": [
    "for w in words[:1]:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95cbc52c-b3d6-45a2-a49c-53ec07afdda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], [] #inputs and targets\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed154f39-4491-44ff-a1c4-492252b2a2ce",
   "metadata": {},
   "source": [
    "### We make the list of inputs and the list of labels tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe9d34d5-c48c-4b45-89e3-13db34fdbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tensors out of this\n",
    "xs = torch.tensor((xs)) #not torch.Tensor(which would cast as float) \n",
    "ys = torch.tensor((ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8aa25c7-0b65-4287-b269-8b9cb7dff09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs #inputs into the neural net for emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b48520a-4f90-44b0-8122-85d50b35c433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys #target/label for next character in sequence for emma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb2db3-186d-4d17-a400-6f6c50dceb45",
   "metadata": {},
   "source": [
    "### Randomly initialize $27$ neurons' weights. Each neuron will receive $27$ inputs\n",
    "\n",
    "- Each character is represented as a 27-dimensional one-hot vector, so the input to the network has 27 features. Thus, the first parameter $27$ in ```torch.randn((27,27)``` represents the number of input features to a neuron, which corresponds to the size of the one-hot encoded vectors.\n",
    "- The second parameter is the number of neurons in the layer ($27$), with each neuron corresponding to a potential next character prediction\n",
    "-  Each column in the ```W matrix``` corresponds to the weights for one neuron. This allows the network to compute the ```weighted sum``` of inputs to determine the activation for each possible next character.\n",
    "-  The network outputs a 27-dimensional vector where each element represents the \"score\" or \"activation\" of a corresponding character being the next in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f10782d4-8e4e-41f8-8d81-680069e4a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27])\n",
      "tensor([[ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
      "          0.0791,  0.9046, -0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,\n",
      "          1.5618, -1.6261,  0.6772, -0.8404,  0.9849, -0.1484, -1.4795,  0.4483,\n",
      "         -0.0707,  2.4968,  2.4448]])\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) #for reproducability\n",
    "W = torch.randn((27,27), generator = g) #27 output neurons with 27 input features each in a one-hot encoded vector, column-wise\n",
    "print(W.shape)\n",
    "print(W[:1]) #example column vector representing 27 input features of a neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b9f136-520c-47a7-800b-9bb4c8119440",
   "metadata": {},
   "source": [
    "### Plug in all the input examples (```xs```) into the neural network and do a forward pass\n",
    "- Each row of ```logits```, or ```log-counts```, represents raw predictions/output values for each character in the final layer of the neural network, before any activation function is applied. They represent the unnormalized prediction scores for each class or output node\n",
    "- ```logits``` are the result of applying any linear transformation to the input data. Usually done by multiplying the input (one-hot encoded input vectors) by a weight matrix ```W```, and adding it to a bias ```b```.\n",
    "- Logits are not probabilities; they can be any real number, positive or negative. The higher the logit value for a particular class, the more the model believes that class is the correct one. However, since logits are not probabilities, they cannot be interpreted directly as confidence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "128c1502-ce18-4caa-9b1f-dc79465fd05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Encoded Shape = torch.Size([5, 27]) \n",
      "X Encoded = tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Logits Shape = torch.Size([5, 27])\n",
      "Logits = tensor([ 4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00,\n",
      "         4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
      "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01,\n",
      "         5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
      "         9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01,\n",
      "        -2.1259e+00,  9.6041e-01])\n"
     ]
    }
   ],
   "source": [
    "#encode all of the inputs into one-hot representations. xencoded is an array of 5x27 with mostly 0's\n",
    "xencoded = F.one_hot(xs, num_classes=27).float() #input to network: one hot encoding\n",
    "print(f'X Encoded Shape = {xencoded.shape} \\nX Encoded = {xencoded[1]}')\n",
    "\n",
    "# We then multiply this in the first layer of the neural net (27x27) to get logits; 5x27 x 27x27 = 5x27\n",
    "# Evaluate all 27 neurons on all 5 input vectors in parallel:\n",
    "# Telling us: what is the firing rate/activation for the 27 neurons we made on all 5 of our input vectors?\n",
    "logits = xencoded @ W #predict log-counts or logits\n",
    "print(f'Logits Shape = {logits.shape}\\nLogits = {logits[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2568bc7-a99e-4775-9157-5216d4d51c8e",
   "metadata": {},
   "source": [
    "### Looking at ```logits``` output above:\n",
    "- Each value corresponds to a character: There are 27 values here, corresponding to the 27 unique characters in ```words```\n",
    "- Each logit represents the model's raw prediction score for the next character in the sequence given the current character\n",
    "\n",
    "### Interpreting a Few Values:\n",
    "- The highest logit value here is 2.3982 for the fifth class. This suggests that given the input, the model predicts the fifth character (based on the one-hot encoding order) is the most likely next character. This is the letter ```e```.\n",
    "- Negative values like -6.5650e-01 or -2.1259e+00 represent low confidence predictions for those classes, implying they are less likely to be the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab30b0c-b3cd-4704-a476-d781bf0c5636",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "To convert logits into probabilities, you typically apply a softmax function. This converts raw scores to probabilites that sum to $1$.\n",
    "\n",
    "This function exponentiates each logit to get counts, normalizes these exponentiated values (counts) by dividing by their sum, and ensures that the probabilities for all classes sum to $1$.\n",
    "\n",
    "### Exponentiation of Logits:\n",
    "\n",
    "Purpose: This step transforms the logits into positive numbers, called ```unnormalized probabilities``` or ```counts,``` which helps in converting the raw scores into a more interpretable form\n",
    "\n",
    "For each logit $z_i$, the exponential transformation is given by $exp(z_i)$\n",
    "\n",
    "\n",
    "### Normalization:\n",
    "\n",
    "Purpose: After exponentiation, the values are normalized by dividing each \"count\" by the sum of all \"counts\" in the tensor. This ensures that the resulting values form a valid probability distribution, i.e., they sum to 1.\n",
    "\n",
    "Formula : \n",
    "$$\n",
    "P(y = i) = \\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "This does:\n",
    "\n",
    "- ```Row-wise Summation```: ```counts.sum(1, keepdim=True)``` calculates the sum of each row, producing a $(5, 1)$ tensor. Each element in this result is the sum of one row from the counts tensor.\n",
    "\n",
    "- ```Broadcasted Division```: The division ```counts / counts.sum(1, keepdim=True)``` then divides each element in a row by the corresponding sum for that row, effectively normalizing each row to produce a valid probability distribution where the probabilities of all classes for a given sample sum to $1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "237ba6f6-43a7-413e-a842-9d11183c5312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6038,  4.4060,  1.3737,  2.8830, 11.0032,  1.5972,  0.5187,  1.8527,\n",
      "         0.5369,  1.6654,  3.8818,  1.2642,  0.6339,  0.9987,  0.5995,  1.7432,\n",
      "         1.6073,  0.2499,  5.0680,  1.1876,  2.6871,  1.6596,  2.7728,  0.1486,\n",
      "         0.6521,  0.1193,  2.6128])\n",
      "tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "torch.Size([5, 27])\n"
     ]
    }
   ],
   "source": [
    "#exponentiate the logits to get fake counts\n",
    "counts = logits.exp() #counts (equivalent to N in prev. model)\n",
    "print(counts[1])\n",
    "\n",
    "#normalize the counts to get probabilities\n",
    "probs = counts/counts.sum(1, keepdim=True) #probabilities for next character\n",
    "print(probs[1])\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8759fc-26c5-4af7-835a-102105e0d7bc",
   "metadata": {},
   "source": [
    "### Normalize the counts to get probability distribution over possible next characters\n",
    "\n",
    "- The ```counts.sum(1, keepdim=True)``` computes the sum of counts across the dimension specified (in this case, $1$ refers to the row in the 2D logits tensor)\n",
    "\n",
    "- The first dimension (axis 0) has a size of $5$, which corresponds to the number of input sequences we're processing in parallel. The second dimension (axis $1$) has a size of $27$, which corresponds to the number of possible characters in the character set.\n",
    "\n",
    "- When you perform a sum operation along a specific axis, you are effectively collapsing that dimension by summing across its elements:\n",
    "\n",
    "- The paramter on which we passed $1$ represents the axis on which the summation operation is performed on the tensor. \n",
    "\n",
    "- ```keepdim=True``` ensures that the resulting tensor maintains the same dimensions for broadcasting purposes.\n",
    "\n",
    "- Example: Continuing from the previous example, the sum of counts would be $7.389 + 2.718 + 1.105 = 11.212$. The probabilities would then be $[7.389/11.212, 2.718/11.212, 1.105/11.212] = [0.659, 0.242, 0.099]$.\n",
    "\n",
    "\n",
    "### Keepdim=True:\n",
    "\n",
    "```keepdim=True``` ensures the result retains the original two-dimensional shape with a size of $1$ for the summed dimension. This allows you to perform ```element-wise division``` of ```counts``` by the sum for each row without needing to reshape the result manually. This division will properly broadcast across each element in the row, ensuring the resulting probs tensor has the same shape as counts.\n",
    "\n",
    "The ```keepdim=True``` parameter in ```PyTorch``` is used when performing reduction operations (like sum, mean, max, etc.) to control the dimensionality of the output tensor.\n",
    "\n",
    "When you perform operations that reduce the number of dimensions (e.g., summing over one axis), the resulting tensor typically has fewer dimensions than the original tensor. However, in certain situations, maintaining the dimensionality can be beneficial for further tensor operations, especially when broadcasting is involved\n",
    "\n",
    "It's just making sure the output tensor can be easily broadcast against other tensors of the same initial dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ccd817f1-6602-4547-bbbb-e8582eac6d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 13, 13,  1])\n",
      "tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "print(xs) #., e, m, m, a\n",
    "print(ys) #e, m, m, a, ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834fd82-1785-42b1-8416-34612bf4ac2b",
   "metadata": {},
   "source": [
    "### $5$ example breaking down ```emma```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "028bf361-24e6-4f9d-b275-3f5d3f1d3b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0.])\n",
      "--------\n",
      "bigram example 1: .e (indices 0,5)\n",
      "input to the neural net:  0\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character):  5\n",
      " probability assigned by the neural net to the correct character: 0.0123\n",
      "log likelihood : -4.3993\n",
      "negative log likelihood:  4.399273872375488\n",
      "--------\n",
      "bigram example 2: em (indices 5,13)\n",
      "input to the neural net:  5\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character):  13\n",
      " probability assigned by the neural net to the correct character: 0.0181\n",
      "log likelihood : -4.0146\n",
      "negative log likelihood:  4.014570713043213\n",
      "--------\n",
      "bigram example 3: mm (indices 13,13)\n",
      "input to the neural net:  13\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character):  13\n",
      " probability assigned by the neural net to the correct character: 0.0267\n",
      "log likelihood : -3.6234\n",
      "negative log likelihood:  3.623408794403076\n",
      "--------\n",
      "bigram example 4: ma (indices 13,1)\n",
      "input to the neural net:  13\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character):  1\n",
      " probability assigned by the neural net to the correct character: 0.0737\n",
      "log likelihood : -2.6081\n",
      "negative log likelihood:  2.6080665588378906\n",
      "--------\n",
      "bigram example 5: a. (indices 1,0)\n",
      "input to the neural net:  1\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character):  0\n",
      " probability assigned by the neural net to the correct character: 0.0150\n",
      "log likelihood : -4.2012\n",
      "negative log likelihood:  4.201204299926758\n",
      "===============\n",
      "AVERAGE LOSS / NEGATIVE LOSS LIKELIHOOD: 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5) #stores the negative log likelihood for each bigram in the sequence\n",
    "print(nlls)\n",
    "for i in range(5):\n",
    "    #i-th bigram:\n",
    "    x = xs[i].item() #input character index in xs\n",
    "    y = ys[i].item() #label character index in ys\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indices {x},{y})')\n",
    "    print('input to the neural net: ', x)\n",
    "\n",
    "    #probabilitiy distribution over the next character for the ith input character\n",
    "    print('output probabilities from the neural net: \\n', probs[i]) # 27 probabilities for all 5 input tensors\n",
    "    print('label (actual next character): ', y)\n",
    "    p = probs[i, y] #p = the probability assigned by the neural network to the correct next character\n",
    "    print(f' probability assigned by the neural net to the correct character: {p.item():.4f}')\n",
    "\n",
    "    #log probabilities are easier to sum for a sequence of characters than raw probabilities, which would require multiplication\n",
    "    logp = torch.log(p) #if p is close to 0, log likelihood will be a low negative number, else close to 0\n",
    "    print(f'log likelihood : {logp.item():.4f}') # a numerically stable/readable p\n",
    "\n",
    "    #loss function -> we want to minimize the negative log likelihood\n",
    "    nll = -logp\n",
    "    print('negative log likelihood: ', nll.item()) #negative log likelihood -> large positive num bad, close to 0 good\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('===============')\n",
    "print(f'AVERAGE LOSS / NEGATIVE LOSS LIKELIHOOD: {nlls.mean().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6493bd-a060-424f-b291-4d995378df2f",
   "metadata": {},
   "source": [
    "### This is a good summary of what we've done so far. Unfortunately, we didn't get lucky with our set of parameters in ```W```. Fortunately, we can change ```W``` by resampling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb158fe3-c25e-4ded-8619-c51009cf13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 1) #adjust seed by 1, which will cause a different W\n",
    "W = torch.randn((27,27), generator = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "035e081f-38cf-4af4-8dcb-d2c60c82e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xencoded = F.one_hot(xs, num_classes=27).float() \n",
    "logits = xencoded @ W #predict log-counts or logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e107c282-243f-4d32-bee4-f419ed4906bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp() \n",
    "probs = counts/counts.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2756270-0d30-414f-b89f-96e4ce36fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indices 0,5)\n",
      "input to the neural net:  0\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0049, 0.0959, 0.0281, 0.0703, 0.0961, 0.0573, 0.0241, 0.0135, 0.0093,\n",
      "        0.1416, 0.0225, 0.0217, 0.0513, 0.0106, 0.0097, 0.0291, 0.0229, 0.0273,\n",
      "        0.0325, 0.0275, 0.0446, 0.0501, 0.0214, 0.0093, 0.0120, 0.0354, 0.0310])\n",
      "label (actual next character):  5\n",
      " probability assigned by the neural net to the correct character: 0.0573\n",
      "log likelihood : -2.8587\n",
      "negative log likelihood:  2.858668565750122\n",
      "--------\n",
      "bigram example 2: em (indices 5,13)\n",
      "input to the neural net:  5\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0426, 0.0113, 0.0266, 0.0507, 0.2370, 0.0580, 0.0421, 0.0094, 0.0136,\n",
      "        0.0297, 0.0044, 0.0782, 0.1028, 0.0146, 0.0172, 0.0288, 0.0263, 0.0319,\n",
      "        0.0248, 0.0210, 0.0063, 0.0057, 0.0309, 0.0269, 0.0298, 0.0089, 0.0205])\n",
      "label (actual next character):  13\n",
      " probability assigned by the neural net to the correct character: 0.0146\n",
      "log likelihood : -4.2275\n",
      "negative log likelihood:  4.227513313293457\n",
      "--------\n",
      "bigram example 3: mm (indices 13,13)\n",
      "input to the neural net:  13\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0973, 0.0235, 0.2014, 0.0240, 0.0510, 0.0341, 0.0644, 0.0075, 0.0197,\n",
      "        0.0502, 0.0316, 0.0015, 0.0192, 0.0321, 0.0127, 0.0035, 0.0148, 0.0052,\n",
      "        0.0262, 0.0415, 0.0086, 0.0445, 0.0277, 0.0252, 0.1034, 0.0042, 0.0250])\n",
      "label (actual next character):  13\n",
      " probability assigned by the neural net to the correct character: 0.0321\n",
      "log likelihood : -3.4399\n",
      "negative log likelihood:  3.4398529529571533\n",
      "--------\n",
      "bigram example 4: ma (indices 13,1)\n",
      "input to the neural net:  13\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0973, 0.0235, 0.2014, 0.0240, 0.0510, 0.0341, 0.0644, 0.0075, 0.0197,\n",
      "        0.0502, 0.0316, 0.0015, 0.0192, 0.0321, 0.0127, 0.0035, 0.0148, 0.0052,\n",
      "        0.0262, 0.0415, 0.0086, 0.0445, 0.0277, 0.0252, 0.1034, 0.0042, 0.0250])\n",
      "label (actual next character):  1\n",
      " probability assigned by the neural net to the correct character: 0.0235\n",
      "log likelihood : -3.7513\n",
      "negative log likelihood:  3.7513229846954346\n",
      "--------\n",
      "bigram example 5: a. (indices 1,0)\n",
      "input to the neural net:  1\n",
      "output probabilities from the neural net: \n",
      " tensor([0.0737, 0.0209, 0.0476, 0.0551, 0.0063, 0.0142, 0.0167, 0.0051, 0.0122,\n",
      "        0.0116, 0.0320, 0.0533, 0.0237, 0.0118, 0.0198, 0.2460, 0.0092, 0.0353,\n",
      "        0.0055, 0.0383, 0.0065, 0.0263, 0.0173, 0.0184, 0.0208, 0.1495, 0.0231])\n",
      "label (actual next character):  0\n",
      " probability assigned by the neural net to the correct character: 0.0737\n",
      "log likelihood : -2.6081\n",
      "negative log likelihood:  2.608081102371216\n",
      "===============\n",
      "AVERAGE LOSS / NEGATIVE LOSS LIKELIHOOD: 3.3770878314971924\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5) #stores the negative log likelihood for each bigram in the sequence\n",
    "for i in range(5):\n",
    "    #i-th bigram:\n",
    "    x = xs[i].item() #input character index in xs\n",
    "    y = ys[i].item() #label character index in ys\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indices {x},{y})')\n",
    "    print('input to the neural net: ', x)\n",
    "\n",
    "    #probabilitiy distribution over the next character for the ith input character\n",
    "    print('output probabilities from the neural net: \\n', probs[i]) # 27 probabilities for all 5 input tensors\n",
    "    print('label (actual next character): ', y)\n",
    "    p = probs[i, y] #p = the probability assigned by the neural network to the correct next character\n",
    "    print(f' probability assigned by the neural net to the correct character: {p.item():.4f}')\n",
    "\n",
    "    #log probabilities are easier to sum for a sequence of characters than raw probabilities, which would require multiplication\n",
    "    logp = torch.log(p) #if p is close to 0, log likelihood will be a low negative number, else close to 0\n",
    "    print(f'log likelihood : {logp.item():.4f}') # a numerically stable/readable p\n",
    "\n",
    "    #loss function -> we want to minimize the negative log likelihood\n",
    "    nll = -logp\n",
    "    print('negative log likelihood: ', nll.item()) #negative log likelihood -> large positive num bad, close to 0 good\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('===============')\n",
    "print(f'AVERAGE LOSS / NEGATIVE LOSS LIKELIHOOD: {nlls.mean().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6aacb8-dcba-41b6-abeb-7ec2a48a2553",
   "metadata": {},
   "source": [
    "### Loss has gone down by about $.4$ by only changing the seed by $1$! So we can do a guess and check to optimize our neural network, but clearly that's beyond extremely inefficient. But we will start with a random guess and then optimize.\n",
    "\n",
    "### The good news is that our loss function is made up of differentiable operations. And we can minimize the loss by tuning the W's, by computing the gradients of the loss with respect to the W matrices. Then we can tune W to minimize the loss and find a good setting of W using gradient based optimization. If you've worked through ```Micrograd```, much if it will be similar to that. Let's see how that would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04d3e5-b380-4431-81f7-330033a3d37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
